{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d13b0a7-8b9f-4f26-a38a-98c7c80561ea",
   "metadata": {
    "id": "7d13b0a7-8b9f-4f26-a38a-98c7c80561ea"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75eb0a76-3d02-4e4d-8319-d94f056c4544",
   "metadata": {
    "id": "75eb0a76-3d02-4e4d-8319-d94f056c4544"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d387a43-984e-4c04-9df0-78cc70ef501c",
   "metadata": {
    "id": "5d387a43-984e-4c04-9df0-78cc70ef501c"
   },
   "source": [
    "# Convolutional Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d214d968-066e-44f5-9ecf-a8bea756294b",
   "metadata": {
    "id": "d214d968-066e-44f5-9ecf-a8bea756294b"
   },
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    def __init__(self, kernel_num, kernel_size):\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernels = np.random.randn(kernel_num, kernel_size, kernel_size) / (kernel_size ** 2)\n",
    "\n",
    "    def patches_generator(self, image):\n",
    "        image_h, image_w = image.shape\n",
    "        self.image = image\n",
    "\n",
    "        for h in range(image_h - self.kernel_size + 1):\n",
    "            for w in range(image_w - self.kernel_size +1):\n",
    "                patch = image[h:(h+self.kernel_size), w:(w+self.kernel_size)]\n",
    "                yield patch, h, w\n",
    "\n",
    "    def forward_prop(self, image):\n",
    "        image_h, image_w = image.shape\n",
    "        convolution_output = np.zeros((image_h - self.kernel_size + 1, image_w - self.kernel_size + 1, self.kernel_num))\n",
    "        for patch, h, w in self.patches_generator(image):\n",
    "            convolution_output[h, w] = np.sum(patch * self.kernels, axis=(1, 2))\n",
    "\n",
    "        return convolution_output\n",
    "\n",
    "    def back_prop(self, dE_dY, alpha):\n",
    "        dE_dk = np.zeros(self.kernels.shape)\n",
    "        for patch, h, w in self.patches_generator(self.image):\n",
    "            for f in range(self.kernel_num):\n",
    "                dE_dk[f] += patch * dE_dY[h, w, f]\n",
    "        self.kernels -= alpha*dE_dk\n",
    "        return dE_dk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50568aeb-9df7-4ecd-8519-015a883dc2c3",
   "metadata": {
    "id": "50568aeb-9df7-4ecd-8519-015a883dc2c3"
   },
   "source": [
    "# Max Pooling Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c68b165-86ba-4697-b564-dc0cfeb06031",
   "metadata": {
    "id": "2c68b165-86ba-4697-b564-dc0cfeb06031"
   },
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, kernel_size):\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def patches_generator(self, image):\n",
    "        output_h = image.shape[0] // self.kernel_size\n",
    "        output_w = image.shape[1] // self.kernel_size\n",
    "        self.image = image\n",
    "\n",
    "        for h in range(output_h):\n",
    "            for w in range(output_w):\n",
    "                patch = image[(h*self.kernel_size):(h*self.kernel_size+self.kernel_size), (w*self.kernel_size):(w*self.kernel_size+self.kernel_size)]\n",
    "                yield patch, h, w\n",
    "\n",
    "    def forward_prop(self, image):\n",
    "        image_h, image_w, num_kernels = image.shape\n",
    "        max_pooling_output = np.zeros((image_h//self.kernel_size, image_w//self.kernel_size, num_kernels))\n",
    "\n",
    "        for patch, h, w in self.patches_generator(image):\n",
    "            max_pooling_output[h, w] = np.amax(patch, axis=(0, 1))\n",
    "\n",
    "        return max_pooling_output\n",
    "\n",
    "    def back_prop(self, dE_dY):\n",
    "        dE_dk = np.zeros(self.image.shape)\n",
    "        for patch,h,w in self.patches_generator(self.image):\n",
    "            image_h, image_w, num_kernels = patch.shape\n",
    "            max_val = np.amax(patch, axis=(0,1))\n",
    "\n",
    "            for idx_h in range(image_h):\n",
    "                for idx_w in range(image_w):\n",
    "                    for idx_k in range(num_kernels):\n",
    "                        if patch[idx_h,idx_w,idx_k] == max_val[idx_k]:\n",
    "                            dE_dk[h*self.kernel_size+idx_h, w*self.kernel_size+idx_w, idx_k] = dE_dY[h,w,idx_k]\n",
    "            return dE_dk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ea98b-22a2-44f3-9f42-2ee9db921f4c",
   "metadata": {
    "id": "563ea98b-22a2-44f3-9f42-2ee9db921f4c"
   },
   "source": [
    "# Sigmoid Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b4e462-8365-47a1-a26b-dd00e93f0be4",
   "metadata": {
    "id": "c3b4e462-8365-47a1-a26b-dd00e93f0be4"
   },
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self, input_units, output_units):\n",
    "        self.weight = np.random.randn(input_units, output_units)/input_units\n",
    "        self.bias = np.zeros(output_units)\n",
    "\n",
    "    def forward_prop(self, image):\n",
    "        self.original_shape = image.shape\n",
    "        image_flattened = image.flatten()\n",
    "        self.flattened_input = image_flattened\n",
    "        first_output = np.dot(image_flattened, self.weight) + self.bias\n",
    "        self.output = first_output\n",
    "        softmax_output = np.exp(first_output) / np.sum(np.exp(first_output), axis=0)\n",
    "        return softmax_output\n",
    "\n",
    "    def back_prop(self, dE_dY, alpha):\n",
    "        for i, gradient in enumerate(dE_dY):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "            transformation_eq = np.exp(self.output)\n",
    "            S_total = np.sum(transformation_eq)\n",
    "\n",
    "            dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
    "            dY_dZ[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total**2)\n",
    "\n",
    "            dZ_dw = self.flattened_input\n",
    "            dZ_db = 1\n",
    "            dZ_dX = self.weight\n",
    "\n",
    "            dE_dZ = gradient * dY_dZ\n",
    "\n",
    "            dE_dw = dZ_dw[np.newaxis].T @ dE_dZ[np.newaxis]\n",
    "            dE_db = dE_dZ * dZ_db\n",
    "            dE_dX = dZ_dX @ dE_dZ\n",
    "\n",
    "            self.weight -= alpha*dE_dw\n",
    "            self.bias -= alpha*dE_db\n",
    "\n",
    "            return dE_dX.reshape(self.original_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401901c-cf5f-4ef3-8171-ddbdf31ea72b",
   "metadata": {
    "id": "a401901c-cf5f-4ef3-8171-ddbdf31ea72b"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2aa6b54-1588-4467-aaff-abe14a8d0bc1",
   "metadata": {
    "id": "c2aa6b54-1588-4467-aaff-abe14a8d0bc1"
   },
   "outputs": [],
   "source": [
    "def CNN_forward(image, label, layers):\n",
    "    output = image/255.\n",
    "    for layer in layers:\n",
    "        output = layer.forward_prop(output)\n",
    "    # Compute loss (cross-entropy) and accuracy\n",
    "    loss = -np.log(output[label])\n",
    "    accuracy = 1 if np.argmax(output) == label else 0\n",
    "    return output, loss, accuracy\n",
    "\n",
    "def CNN_backprop(gradient, layers, alpha=0.05):\n",
    "    grad_back = gradient\n",
    "    for layer in layers[::-1]:\n",
    "        if type(layer) in [ConvolutionLayer, SoftmaxLayer]:\n",
    "            grad_back = layer.back_prop(grad_back, alpha)\n",
    "        elif type(layer) == MaxPoolingLayer:\n",
    "            grad_back = layer.back_prop(grad_back)\n",
    "    return grad_back\n",
    "\n",
    "\n",
    "def CNN_training(image, label, layers, alpha=0.05):\n",
    "    # Forward step\n",
    "    output, loss, accuracy = CNN_forward(image, label, layers)\n",
    "\n",
    "    # Initial gradient\n",
    "    gradient = np.zeros(10)\n",
    "    gradient[label] = -1/output[label]\n",
    "\n",
    "    # Backprop step\n",
    "    gradient_back = CNN_backprop(gradient, layers, alpha)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "519f5859-6289-41eb-b1f7-e2ff1170bb34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "519f5859-6289-41eb-b1f7-e2ff1170bb34",
    "outputId": "7297bf9f-a3a4-4fe6-fe87-c34f1bbbc9ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n",
      "Epoch 1 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss 1.6460076465605926, accuracy 48\n",
      "Step 201. For the last 100 steps: average loss 1.1764334981336049, accuracy 63\n",
      "Step 301. For the last 100 steps: average loss 0.8234852509826963, accuracy 72\n",
      "Step 401. For the last 100 steps: average loss 0.7753918245773093, accuracy 75\n",
      "Step 501. For the last 100 steps: average loss 0.6718619851247729, accuracy 78\n",
      "Step 601. For the last 100 steps: average loss 0.5937589685994265, accuracy 80\n",
      "Step 701. For the last 100 steps: average loss 0.5938000836231578, accuracy 81\n",
      "Step 801. For the last 100 steps: average loss 0.6660356413439178, accuracy 79\n",
      "Step 901. For the last 100 steps: average loss 0.6492171884644953, accuracy 77\n",
      "Step 1001. For the last 100 steps: average loss 0.416135103526644, accuracy 88\n",
      "Step 1101. For the last 100 steps: average loss 0.5916248653370724, accuracy 83\n",
      "Step 1201. For the last 100 steps: average loss 0.5236332854059436, accuracy 84\n",
      "Step 1301. For the last 100 steps: average loss 0.6743627131125267, accuracy 76\n",
      "Step 1401. For the last 100 steps: average loss 0.5549736444315102, accuracy 81\n",
      "Step 1501. For the last 100 steps: average loss 0.5115794071295093, accuracy 81\n",
      "Step 1601. For the last 100 steps: average loss 0.3833793571296505, accuracy 87\n",
      "Step 1701. For the last 100 steps: average loss 0.38860464350128315, accuracy 85\n",
      "Step 1801. For the last 100 steps: average loss 0.28505837555597263, accuracy 90\n",
      "Step 1901. For the last 100 steps: average loss 0.5373897725708003, accuracy 86\n",
      "Step 2001. For the last 100 steps: average loss 0.40673076295429644, accuracy 88\n",
      "Step 2101. For the last 100 steps: average loss 0.42484689570166945, accuracy 88\n",
      "Step 2201. For the last 100 steps: average loss 0.26600043505357684, accuracy 93\n",
      "Step 2301. For the last 100 steps: average loss 0.6196523770997406, accuracy 83\n",
      "Step 2401. For the last 100 steps: average loss 0.2692028117613132, accuracy 92\n",
      "Step 2501. For the last 100 steps: average loss 0.43688366542265844, accuracy 88\n",
      "Step 2601. For the last 100 steps: average loss 0.36888877188582514, accuracy 88\n",
      "Step 2701. For the last 100 steps: average loss 0.3822667890299333, accuracy 90\n",
      "Step 2801. For the last 100 steps: average loss 0.3940273585491555, accuracy 88\n",
      "Step 2901. For the last 100 steps: average loss 0.581330746514212, accuracy 83\n",
      "Step 3001. For the last 100 steps: average loss 0.5324359004985821, accuracy 88\n",
      "Step 3101. For the last 100 steps: average loss 0.56899553830824, accuracy 86\n",
      "Step 3201. For the last 100 steps: average loss 0.4142179579331751, accuracy 88\n",
      "Step 3301. For the last 100 steps: average loss 0.5852710278457449, accuracy 80\n",
      "Step 3401. For the last 100 steps: average loss 0.5141952246240811, accuracy 86\n",
      "Step 3501. For the last 100 steps: average loss 0.4165569069938192, accuracy 87\n",
      "Step 3601. For the last 100 steps: average loss 0.13434484654858062, accuracy 95\n",
      "Step 3701. For the last 100 steps: average loss 0.3723400073486654, accuracy 87\n",
      "Step 3801. For the last 100 steps: average loss 0.35097847097526547, accuracy 85\n",
      "Step 3901. For the last 100 steps: average loss 0.4570636364138134, accuracy 85\n",
      "Step 4001. For the last 100 steps: average loss 0.40041443073512845, accuracy 86\n",
      "Step 4101. For the last 100 steps: average loss 0.30737248389497407, accuracy 90\n",
      "Step 4201. For the last 100 steps: average loss 0.5774403601317545, accuracy 84\n",
      "Step 4301. For the last 100 steps: average loss 0.31400565106967676, accuracy 92\n",
      "Step 4401. For the last 100 steps: average loss 0.3786727661682799, accuracy 87\n",
      "Step 4501. For the last 100 steps: average loss 0.32179634580979566, accuracy 90\n",
      "Step 4601. For the last 100 steps: average loss 0.30034401504047487, accuracy 94\n",
      "Step 4701. For the last 100 steps: average loss 0.32463798848408365, accuracy 95\n",
      "Step 4801. For the last 100 steps: average loss 0.3503220246777221, accuracy 91\n",
      "Step 4901. For the last 100 steps: average loss 0.5112244915374822, accuracy 84\n",
      "Epoch 2 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss 0.29099361584818245, accuracy 93\n",
      "Step 201. For the last 100 steps: average loss 0.15416443099680172, accuracy 96\n",
      "Step 301. For the last 100 steps: average loss 0.3414065021950227, accuracy 87\n",
      "Step 401. For the last 100 steps: average loss 0.4460486546931339, accuracy 87\n",
      "Step 501. For the last 100 steps: average loss 0.24041817547357552, accuracy 93\n",
      "Step 601. For the last 100 steps: average loss 0.21647814303713325, accuracy 94\n",
      "Step 701. For the last 100 steps: average loss 0.39702586412668983, accuracy 89\n",
      "Step 801. For the last 100 steps: average loss 0.2009947871875962, accuracy 92\n",
      "Step 901. For the last 100 steps: average loss 0.19990594376098414, accuracy 91\n",
      "Step 1001. For the last 100 steps: average loss 0.30919147369100203, accuracy 93\n",
      "Step 1101. For the last 100 steps: average loss 0.1337776345802281, accuracy 96\n",
      "Step 1201. For the last 100 steps: average loss 0.3122583438178809, accuracy 90\n",
      "Step 1301. For the last 100 steps: average loss 0.14756245937185217, accuracy 94\n",
      "Step 1401. For the last 100 steps: average loss 0.26518854641773687, accuracy 92\n",
      "Step 1501. For the last 100 steps: average loss 0.362227470782666, accuracy 86\n",
      "Step 1601. For the last 100 steps: average loss 0.4449802944418608, accuracy 89\n",
      "Step 1701. For the last 100 steps: average loss 0.21251295995867925, accuracy 95\n",
      "Step 1801. For the last 100 steps: average loss 0.30419649958523526, accuracy 91\n",
      "Step 1901. For the last 100 steps: average loss 0.3463551321348485, accuracy 92\n",
      "Step 2001. For the last 100 steps: average loss 0.3485590932852795, accuracy 84\n",
      "Step 2101. For the last 100 steps: average loss 0.1656175501077102, accuracy 95\n",
      "Step 2201. For the last 100 steps: average loss 0.3360976621820581, accuracy 87\n",
      "Step 2301. For the last 100 steps: average loss 0.288452755354003, accuracy 91\n",
      "Step 2401. For the last 100 steps: average loss 0.2991210096720937, accuracy 91\n",
      "Step 2501. For the last 100 steps: average loss 0.3116904827066344, accuracy 90\n",
      "Step 2601. For the last 100 steps: average loss 0.25288605526315755, accuracy 92\n",
      "Step 2701. For the last 100 steps: average loss 0.46747901321949337, accuracy 90\n",
      "Step 2801. For the last 100 steps: average loss 0.34448958401411545, accuracy 90\n",
      "Step 2901. For the last 100 steps: average loss 0.3381476362698518, accuracy 88\n",
      "Step 3001. For the last 100 steps: average loss 0.16265374572009264, accuracy 93\n",
      "Step 3101. For the last 100 steps: average loss 0.1941732750523492, accuracy 96\n",
      "Step 3201. For the last 100 steps: average loss 0.3478322050790126, accuracy 91\n",
      "Step 3301. For the last 100 steps: average loss 0.3109330096556041, accuracy 92\n",
      "Step 3401. For the last 100 steps: average loss 0.45264766233149095, accuracy 89\n",
      "Step 3501. For the last 100 steps: average loss 0.19224871895727516, accuracy 94\n",
      "Step 3601. For the last 100 steps: average loss 0.3468830455697688, accuracy 91\n",
      "Step 3701. For the last 100 steps: average loss 0.4988026089685203, accuracy 84\n",
      "Step 3801. For the last 100 steps: average loss 0.48913370045056204, accuracy 84\n",
      "Step 3901. For the last 100 steps: average loss 0.37884386326153324, accuracy 88\n",
      "Step 4001. For the last 100 steps: average loss 0.2980348329418906, accuracy 92\n",
      "Step 4101. For the last 100 steps: average loss 0.24952776047241101, accuracy 93\n",
      "Step 4201. For the last 100 steps: average loss 0.2533740346375043, accuracy 92\n",
      "Step 4301. For the last 100 steps: average loss 0.4616675622016546, accuracy 88\n",
      "Step 4401. For the last 100 steps: average loss 0.46365397673794734, accuracy 87\n",
      "Step 4501. For the last 100 steps: average loss 0.3800233447366923, accuracy 88\n",
      "Step 4601. For the last 100 steps: average loss 0.4247959949421143, accuracy 87\n",
      "Step 4701. For the last 100 steps: average loss 0.33347241681516443, accuracy 90\n",
      "Step 4801. For the last 100 steps: average loss 0.3411068610631656, accuracy 90\n",
      "Step 4901. For the last 100 steps: average loss 0.30935645623907626, accuracy 94\n",
      "Epoch 3 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss 0.22304225145332476, accuracy 93\n",
      "Step 201. For the last 100 steps: average loss 0.22811864656992578, accuracy 94\n",
      "Step 301. For the last 100 steps: average loss 0.3240387336290331, accuracy 92\n",
      "Step 401. For the last 100 steps: average loss 0.3448184121232443, accuracy 88\n",
      "Step 501. For the last 100 steps: average loss 0.32350538539909673, accuracy 96\n",
      "Step 601. For the last 100 steps: average loss 0.1879554500162303, accuracy 95\n",
      "Step 701. For the last 100 steps: average loss 0.3011522794219523, accuracy 88\n",
      "Step 801. For the last 100 steps: average loss 0.2827968473044078, accuracy 93\n",
      "Step 901. For the last 100 steps: average loss 0.18433996992766338, accuracy 94\n",
      "Step 1001. For the last 100 steps: average loss 0.2621436600732905, accuracy 91\n",
      "Step 1101. For the last 100 steps: average loss 0.21716972002188123, accuracy 94\n",
      "Step 1201. For the last 100 steps: average loss 0.23297506063546738, accuracy 92\n",
      "Step 1301. For the last 100 steps: average loss 0.26717215490500945, accuracy 93\n",
      "Step 1401. For the last 100 steps: average loss 0.4125097200097956, accuracy 91\n",
      "Step 1501. For the last 100 steps: average loss 0.34907509763891076, accuracy 92\n",
      "Step 1601. For the last 100 steps: average loss 0.14686467494677424, accuracy 96\n",
      "Step 1701. For the last 100 steps: average loss 0.18474107254056668, accuracy 95\n",
      "Step 1801. For the last 100 steps: average loss 0.37340769577184285, accuracy 85\n",
      "Step 1901. For the last 100 steps: average loss 0.25655631458604655, accuracy 95\n",
      "Step 2001. For the last 100 steps: average loss 0.16492419792958476, accuracy 96\n",
      "Step 2101. For the last 100 steps: average loss 0.3131010857410771, accuracy 90\n",
      "Step 2201. For the last 100 steps: average loss 0.42781528740813335, accuracy 89\n",
      "Step 2301. For the last 100 steps: average loss 0.26336289341093894, accuracy 92\n",
      "Step 2401. For the last 100 steps: average loss 0.4497991704692964, accuracy 89\n",
      "Step 2501. For the last 100 steps: average loss 0.2362066617430848, accuracy 93\n",
      "Step 2601. For the last 100 steps: average loss 0.141899141000897, accuracy 97\n",
      "Step 2701. For the last 100 steps: average loss 0.17619008591046204, accuracy 95\n",
      "Step 2801. For the last 100 steps: average loss 0.2982911957438314, accuracy 89\n",
      "Step 2901. For the last 100 steps: average loss 0.25736224909866806, accuracy 90\n",
      "Step 3001. For the last 100 steps: average loss 0.5575528710701676, accuracy 89\n",
      "Step 3101. For the last 100 steps: average loss 0.3369546063974343, accuracy 89\n",
      "Step 3201. For the last 100 steps: average loss 0.37160136537729527, accuracy 86\n",
      "Step 3301. For the last 100 steps: average loss 0.21474261411296858, accuracy 94\n",
      "Step 3401. For the last 100 steps: average loss 0.2349129628373821, accuracy 93\n",
      "Step 3501. For the last 100 steps: average loss 0.25654877979601987, accuracy 93\n",
      "Step 3601. For the last 100 steps: average loss 0.16348139725534927, accuracy 92\n",
      "Step 3701. For the last 100 steps: average loss 0.3855926797399302, accuracy 88\n",
      "Step 3801. For the last 100 steps: average loss 0.14119881616736735, accuracy 94\n",
      "Step 3901. For the last 100 steps: average loss 0.10588643847403931, accuracy 98\n",
      "Step 4001. For the last 100 steps: average loss 0.15525983826276876, accuracy 94\n",
      "Step 4101. For the last 100 steps: average loss 0.2517304555282931, accuracy 94\n",
      "Step 4201. For the last 100 steps: average loss 0.44573570674615626, accuracy 89\n",
      "Step 4301. For the last 100 steps: average loss 0.23929350243667857, accuracy 92\n",
      "Step 4401. For the last 100 steps: average loss 0.2647686864888273, accuracy 92\n",
      "Step 4501. For the last 100 steps: average loss 0.32896664321877883, accuracy 87\n",
      "Step 4601. For the last 100 steps: average loss 0.37927411749449547, accuracy 89\n",
      "Step 4701. For the last 100 steps: average loss 0.14724042113628577, accuracy 94\n",
      "Step 4801. For the last 100 steps: average loss 0.1623644084515284, accuracy 95\n",
      "Step 4901. For the last 100 steps: average loss 0.2772575930961897, accuracy 91\n",
      "Epoch 4 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss 0.17415959000653178, accuracy 95\n",
      "Step 201. For the last 100 steps: average loss 0.22322006432702565, accuracy 95\n",
      "Step 301. For the last 100 steps: average loss 0.2842400024888994, accuracy 94\n",
      "Step 401. For the last 100 steps: average loss 0.26811612381265, accuracy 95\n",
      "Step 501. For the last 100 steps: average loss 0.08627393868902482, accuracy 98\n",
      "Step 601. For the last 100 steps: average loss 0.19330117949880635, accuracy 92\n",
      "Step 701. For the last 100 steps: average loss 0.2741811505912137, accuracy 93\n",
      "Step 801. For the last 100 steps: average loss 0.23196591679132816, accuracy 94\n",
      "Step 901. For the last 100 steps: average loss 0.19049210670635694, accuracy 97\n",
      "Step 1001. For the last 100 steps: average loss 0.239679037416873, accuracy 93\n",
      "Step 1101. For the last 100 steps: average loss 0.13310100175553027, accuracy 97\n",
      "Step 1201. For the last 100 steps: average loss 0.27000670723859127, accuracy 91\n",
      "Step 1301. For the last 100 steps: average loss 0.2914952913251738, accuracy 91\n",
      "Step 1401. For the last 100 steps: average loss 0.3582749398482948, accuracy 92\n",
      "Step 1501. For the last 100 steps: average loss 0.20713613763208208, accuracy 92\n",
      "Step 1601. For the last 100 steps: average loss 0.322444977348697, accuracy 90\n",
      "Step 1701. For the last 100 steps: average loss 0.160348297460763, accuracy 96\n",
      "Step 1801. For the last 100 steps: average loss 0.14181967264000833, accuracy 95\n",
      "Step 1901. For the last 100 steps: average loss 0.3109261221368486, accuracy 90\n",
      "Step 2001. For the last 100 steps: average loss 0.30357764947024246, accuracy 90\n",
      "Step 2101. For the last 100 steps: average loss 0.4039186039703742, accuracy 90\n",
      "Step 2201. For the last 100 steps: average loss 0.24273093578681937, accuracy 93\n",
      "Step 2301. For the last 100 steps: average loss 0.26985369339630816, accuracy 90\n",
      "Step 2401. For the last 100 steps: average loss 0.23510636868329357, accuracy 91\n",
      "Step 2501. For the last 100 steps: average loss 0.37231319850448935, accuracy 89\n",
      "Step 2601. For the last 100 steps: average loss 0.3828153704149235, accuracy 90\n",
      "Step 2701. For the last 100 steps: average loss 0.19333375177790227, accuracy 94\n",
      "Step 2801. For the last 100 steps: average loss 0.12053625478736026, accuracy 95\n",
      "Step 2901. For the last 100 steps: average loss 0.23229908635133317, accuracy 94\n",
      "Step 3001. For the last 100 steps: average loss 0.17870949001067493, accuracy 97\n",
      "Step 3101. For the last 100 steps: average loss 0.2041910382058495, accuracy 95\n",
      "Step 3201. For the last 100 steps: average loss 0.07249948829839165, accuracy 97\n",
      "Step 3301. For the last 100 steps: average loss 0.14538865616984847, accuracy 96\n",
      "Step 3401. For the last 100 steps: average loss 0.21375178224123265, accuracy 92\n",
      "Step 3501. For the last 100 steps: average loss 0.1866025689200505, accuracy 94\n",
      "Step 3601. For the last 100 steps: average loss 0.24207008184675793, accuracy 90\n",
      "Step 3701. For the last 100 steps: average loss 0.13796275788988793, accuracy 97\n",
      "Step 3801. For the last 100 steps: average loss 0.3318012447134725, accuracy 90\n",
      "Step 3901. For the last 100 steps: average loss 0.2674233527643281, accuracy 89\n",
      "Step 4001. For the last 100 steps: average loss 0.20618265799960792, accuracy 93\n",
      "Step 4101. For the last 100 steps: average loss 0.40385397887301033, accuracy 92\n",
      "Step 4201. For the last 100 steps: average loss 0.18985619421451566, accuracy 94\n",
      "Step 4301. For the last 100 steps: average loss 0.23004410561932553, accuracy 95\n",
      "Step 4401. For the last 100 steps: average loss 0.11531132887735332, accuracy 96\n",
      "Step 4501. For the last 100 steps: average loss 0.1271101326314208, accuracy 96\n",
      "Step 4601. For the last 100 steps: average loss 0.15616871356562462, accuracy 97\n",
      "Step 4701. For the last 100 steps: average loss 0.16456750827099306, accuracy 96\n",
      "Step 4801. For the last 100 steps: average loss 0.2787771515759189, accuracy 92\n",
      "Step 4901. For the last 100 steps: average loss 0.23468589356752015, accuracy 94\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ret_loss = []\n",
    "    ret_acc = []\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    X_train = X_train[:5000]\n",
    "    y_train = y_train[:5000]\n",
    "\n",
    "    layers = [\n",
    "    ConvolutionLayer(16,3), # layer with 16 3x3 filters, output (26,26,16)\n",
    "    MaxPoolingLayer(2), # pooling layer 2x2, output (13,13,16)\n",
    "    SoftmaxLayer(13*13*16, 10) # softmax layer with 13*13*16 input and 10 output\n",
    "    ]\n",
    "\n",
    "    for epoch in range(4):\n",
    "        print('Epoch {} ->'.format(epoch+1))\n",
    "\n",
    "        permutation = np.random.permutation(len(X_train))\n",
    "        X_train = X_train[permutation]\n",
    "        y_train = y_train[permutation]\n",
    "\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        for i, (image, label) in enumerate(zip(X_train, y_train)):\n",
    "            if i % 100 == 0: # Every 100 examples\n",
    "                print(\"Step {}. For the last 100 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
    "                ret_loss.append(loss/100)\n",
    "                ret_acc.append(accuracy)\n",
    "                loss = 0\n",
    "                accuracy = 0\n",
    "            loss_1, accuracy_1 = CNN_training(image, label, layers)\n",
    "            loss += loss_1\n",
    "            accuracy += accuracy_1\n",
    "\n",
    "    return ret_loss, ret_acc\n",
    "\n",
    "loss, acc = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6vDbAuh60cdQ",
   "metadata": {
    "id": "6vDbAuh60cdQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
